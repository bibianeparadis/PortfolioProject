{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dcad8f-19b3-4aaf-b5e3-e4198a2e995e",
   "metadata": {},
   "source": [
    "## From section 8.3 of the textbook solve the following exercises:\n",
    "Q1\n",
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8d0036-5a4c-4dbc-9b4a-948bddf0b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad data for Nana\n",
      "Number of Family movies: 2767\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"movies_metadata.csv\", encoding=\"utf-8\")\n",
    "\n",
    "movie_list = []\n",
    "genre_list = []\n",
    "\n",
    "#function to extract genres from each row\n",
    "def map_genres(row):\n",
    "    try:\n",
    "        glist = eval(row.genres)   #genres column has a list-like string\n",
    "    except:\n",
    "        glist = []\n",
    "        print(f\"bad data for {row.title}\")\n",
    "    for g in glist:\n",
    "        movie_list.append(row.imdb_id)\n",
    "        genre_list.append(g['name'])\n",
    "\n",
    "#apply function across all rows\n",
    "_ = df.apply(map_genres, axis=1)\n",
    "\n",
    "#create tidy dataframe of movieâ€“genre pairs\n",
    "movies_genres = pd.DataFrame({\n",
    "    'imdb_id': movie_list,\n",
    "    'genre': genre_list\n",
    "})\n",
    "\n",
    "#Q-1: How many movies are in the Family genre?\n",
    "family_movies_count = movies_genres[movies_genres['genre'] == 'Family']['imdb_id'].nunique()\n",
    "print(\"Number of Family movies:\", family_movies_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b33ce0-39b9-4246-8dd7-810299b11a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common genre: Drama with 20236 movies\n"
     ]
    }
   ],
   "source": [
    "# Q-2: Which genre has the most movies?\n",
    "genre_counts = movies_genres.groupby('genre')['imdb_id'].nunique()\n",
    "most_common_genre = genre_counts.idxmax()\n",
    "most_common_count = genre_counts.max()\n",
    "\n",
    "print(\"Most common genre:\", most_common_genre, \"with\", most_common_count, \"movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc1396-ea70-4f3e-b0ec-a2ec80104822",
   "metadata": {},
   "source": [
    "## From section 8.5 of the textbook solve the  following exercises:\n",
    "Q1\n",
    "Q2\n",
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c5b894-51a8-4a36-b70e-95f632a5a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with country code starting with M: 663\n",
      "Unique country codes starting with M: 18\n",
      "Most common word after 'global': economic\n",
      "Occurrences: 1038\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "undf = pd.read_csv(\"C:/Users/bibia/Downloads/un-general-debates.csv\", encoding=\"utf-8\")\n",
    "\n",
    "#clean up text\n",
    "undf['text'] = undf['text'].str.replace('\\ufeff','', regex=True)  # remove strange char\n",
    "undf['text'] = undf['text'].str.strip()  # remove leading/trailing whitespace\n",
    "\n",
    "#Q-1: How many rows have a country code starting with 'M'?\n",
    "rows_starting_M = undf[undf['country'].str.startswith('M')].shape[0]\n",
    "print(\"Rows with country code starting with M:\", rows_starting_M)\n",
    "\n",
    "#Q-2: How many unique country codes start with 'M'?\n",
    "codes_starting_M = undf[undf['country'].str.startswith('M')]['country'].nunique()\n",
    "print(\"Unique country codes starting with M:\", codes_starting_M)\n",
    "\n",
    "#Q-3: Most common word that follows 'global'\n",
    "#lowercase text\n",
    "undf['text'] = undf['text'].str.lower()\n",
    "\n",
    "#extract ALL words following 'global'\n",
    "following_words = undf['text'].str.extractall(r'global\\s+(\\w+)')[0]\n",
    "\n",
    "word_counts = following_words.value_counts()\n",
    "\n",
    "#most common word and how many times it occurs\n",
    "most_common_word = word_counts.idxmax()\n",
    "most_common_count = word_counts.max()\n",
    "\n",
    "print(\"Most common word after 'global':\", most_common_word)\n",
    "print(\"Occurrences:\", most_common_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e03954-31ac-4119-8539-b8bdff492f9b",
   "metadata": {},
   "source": [
    "## From section 8.6 of the textbook solve the  following exercises:\n",
    "Q1\n",
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a577b4-60c8-495f-89df-154c516072bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of MEX combined string: 19202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "undf = pd.read_csv(\"C:/Users/bibia/Downloads/un-general-debates.csv\", encoding=\"utf-8\")\n",
    "\n",
    "#Q-1: What is the length of the resulting string for MEX?\n",
    "#keep only 2014 and 2015\n",
    "undf_filtered = undf[undf['year'].isin([2014, 2015])]\n",
    "\n",
    "countries = ['USA', 'CAN', 'CUB', 'MEX']\n",
    "undf_filtered = undf_filtered[undf_filtered['country'].isin(countries)]\n",
    "\n",
    "#combine all speeches for each country into one string\n",
    "# Using sum() to concatenate text\n",
    "combined_texts = undf_filtered.groupby('country')['text'].sum()\n",
    "\n",
    "\n",
    "#add country names as new column\n",
    "country_names = {'USA': 'United States', 'CAN': 'Canada', 'CUB': 'Cuba', 'MEX': 'Mexico'}\n",
    "combined_df = combined_texts.reset_index()\n",
    "combined_df['country_name'] = combined_df['country'].map(country_names)\n",
    "\n",
    "mex_length = len(combined_texts['MEX'])\n",
    "print(\"Length of MEX combined string:\", mex_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d7876d-b48b-4dd6-8ffa-bcdd593cd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexico refers to Canada since 2000: 3\n"
     ]
    }
   ],
   "source": [
    "#Q-2: If we consider all of the years starting with 2000 and after, how many times does Mexico refer to Canada?\n",
    "\n",
    "#filter years 2000 and later\n",
    "undf_2000 = undf[undf['year'] >= 2000]\n",
    "undf_2000 = undf_2000[undf_2000['country'].isin(countries)]\n",
    "\n",
    "#combine texts by country\n",
    "combined_texts_2000 = undf_2000.groupby('country')['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "mex_to_can = combined_texts_2000['MEX'].count(\"Canada\")\n",
    "print(\"Mexico refers to Canada since 2000:\", mex_to_can)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf486a9-e785-4d5e-b394-3a4cbd10cda9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bbb90-4809-49c4-8e24-071272a3e162",
   "metadata": {},
   "source": [
    "#### Step 0.\n",
    "Load the data from un-general-debates.csv into a data frame. Prepare the data and libraries for generating histograms and performing sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d407e6-fc5f-4fb8-9758-a06379e4cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\bibia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/bibia/Downloads/un-general-debates.csv\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#add sentiment score column\n",
    "df[\"sentiment\"] = df[\"text\"].apply(lambda x: sia.polarity_scores(str(x))[\"compound\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f473684-e3c4-49a5-b728-188b11722783",
   "metadata": {},
   "source": [
    "#### Step 1.\n",
    "Generate the collective histogram of sentiment of all UN speeches overs years since 2003. This restriction is to allow google collab to successfully complete the computations. Make a copy of the UN dataframe and modify it as necessary to complete this task. You will not need to try any of the other tricks the book indicates to allow colab to complete the computation if you limit the data to just these years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b74e4-82df-4244-b14f-51c6f68ebc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set speeches from 2003 and onward\n",
    "df_2003 = df[df[\"year\"] >= 2003]\n",
    "\n",
    "#histogram\n",
    "plt.hist(df_2003[\"sentiment\"], bins=30, color=\"blue\", edgecolor=\"black\")\n",
    "plt.title(\"Sentiment of UN Speeches (2003+)\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fda56-2caa-479e-b206-0631d47ab6c2",
   "metadata": {},
   "source": [
    "#### Step 2.\n",
    "\n",
    "Generate histogram over the years for 'sentiment'. Create another copy of the UN dataframe but with only 'year' and 'sentiment' as columns. Calculate the average sentiment for each year for the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bac5fd-d5a2-4ecc-9714-55fa2a118ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep year and sentiment columns, group by year and avg sentiment\n",
    "df_yearly = df_2003[[\"year\", \"sentiment\"]].groupby(\"year\").mean().reset_index()\n",
    "\n",
    "#bar chart\n",
    "plt.bar(df_yearly[\"year\"], df_yearly[\"sentiment\"], color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Average Sentiment of UN Speeches by Year (2003+)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Sentiment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c775aa-fe0d-400b-9010-290d39cf291f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
